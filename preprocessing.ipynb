{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib._GeneratorContextManager at 0x2226029e5c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import sklearn\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "sklearn.config_context(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', low_memory=False)\n",
    "test = pd.read_csv('test.csv', low_memory=False)\n",
    "submission = pd.read_csv('sample_submission.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_map = {'UA': -1, 'HU06': 0, 'HU07': 1, 'HU11': 2,'HU12': 3, 'HU14': 4, 'HU15': 5, 'HU19': 6}\n",
    "train[\"LABEL\"] = train[\"LABEL\"].map(encoding_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Türkiye'deki şehirleri ve bunların ait olduğu bölgeleri içeren bir sözlük oluşturduk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_to_region = {\n",
    "    0: \"Yurtdisi\", 1: \"Akdeniz\", 2: \"Marmara\", 3: \"Ege\", 4: \"Doğu Anadolu\", 5: \"Akdeniz\",\n",
    "    6: \"İç Anadolu\", 7: \"Akdeniz\", 8: \"Karadeniz\", 9: \"Ege\", 10: \"Ege\",\n",
    "    11: \"İç Anadolu\", 12: \"Karadeniz\", 13: \"Marmara\", 14: \"Karadeniz\", 15: \"Marmara\",\n",
    "    16: \"Marmara\", 17: \"Karadeniz\", 18: \"İç Anadolu\", 19: \"Karadeniz\", 20: \"Ege\",\n",
    "    21: \"Doğu Anadolu\", 22: \"Ege\", 23: \"Doğu Anadolu\", 24: \"Doğu Anadolu\", 25: \"Doğu Anadolu\",\n",
    "    26: \"Ege\", 27: \"Akdeniz\", 28: \"Marmara\", 29: \"Karadeniz\", 30: \"Akdeniz\",\n",
    "    31: \"Akdeniz\", 32: \"Akdeniz\", 33: \"Marmara\", 34: \"Marmara\", 35: \"Ege\",\n",
    "    36: \"Doğu Anadolu\", 37: \"Karadeniz\", 38: \"İç Anadolu\", 39: \"Marmara\", 40: \"İç Anadolu\",\n",
    "    41: \"Marmara\", 42: \"İç Anadolu\", 43: \"Ege\", 44: \"Doğu Anadolu\", 45: \"Ege\",\n",
    "    46: \"Akdeniz\", 47: \"Güneydoğu Anadolu\", 48: \"Ege\", 49: \"Doğu Anadolu\", 50: \"İç Anadolu\",\n",
    "    51: \"Doğu Anadolu\", 52: \"Karadeniz\", 53: \"Karadeniz\", 54: \"Marmara\", 55: \"Karadeniz\",\n",
    "    56: \"Güneydoğu Anadolu\", 57: \"Karadeniz\", 58: \"İç Anadolu\", 59: \"Marmara\", 60: \"Karadeniz\",\n",
    "    61: \"Karadeniz\", 62: \"Güneydoğu Anadolu\", 63: \"Güneydoğu Anadolu\", 64: \"Ege\",\n",
    "    65: \"Doğu Anadolu\", 66: \"İç Anadolu\", 67: \"Karadeniz\", 68: \"İç Anadolu\", 69: \"Doğu Anadolu\",\n",
    "    70: \"Akdeniz\", 71: \"İç Anadolu\", 72: \"Güneydoğu Anadolu\", 73: \"Doğu Anadolu\",\n",
    "    74: \"Karadeniz\", 75: \"Doğu Anadolu\", 76: \"Doğu Anadolu\", 77: \"Marmara\", 78: \"Karadeniz\",\n",
    "    79: \"Doğu Anadolu\", 80: \"Akdeniz\", 81: \"Karadeniz\"\n",
    "}\n",
    "\n",
    "all_data['IL'] = all_data['IL'].fillna(34).astype(\"category\")\n",
    "all_data['Regions'] = all_data['IL'].map(city_to_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mapping = {\n",
    "    'OCAK': 1,\n",
    "    'SUBAT': 2,\n",
    "    'MART': 3,\n",
    "    'NISAN': 4,\n",
    "    'MAYIS': 5,\n",
    "    'HAZIRAN': 6,\n",
    "    'TEMMUZ': 7,\n",
    "    'AGUSTOS': 8,\n",
    "    'EYLUL': 9,\n",
    "    'EKIM': 10,\n",
    "    'KASIM': 11,\n",
    "    'ARALIK': 12\n",
    "    }\n",
    "\n",
    "# FLAG sütununu map fonksiyonu ile encode edin\n",
    "all_data['FLAG'] = all_data['FLAG'].map(month_mapping).astype(int)\n",
    "all_data.sort_values(by='FLAG', ascending=True, inplace=True)\n",
    "all_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"SORU_GELIR_CVP\"] = all_data[\"SORU_GELIR_CVP\"].apply(lambda x: str(x).replace(\",\", \".\")).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['PP_UYRUK', 'SORU_MEDENI_HAL_CVP', 'SORU_YATIRIM_KARAKTERI_CVP', 'SORU_EGITIM_CVP', 'PP_MUSTERI_SEGMENTI', 'Regions', 'PP_MESLEK']\n",
    "\n",
    "num_cols = ['PP_YAS', 'SORU_YATIRIM_KARAKTERI_RG', 'SORU_EGITIM_RG', 'SORU_GELIR_CVP', 'SORU_GELIR_RG', 'SORU_COCUK_SAYISI_RG', \n",
    "            'SORU_MEDENI_HAL_RG', 'SORU_COCUK_SAYISI_CVP','BES_AYRILMA_TALEP_ADET', 'ODEMEME_TALEP_ADET', 'HAYAT_AYRILMA_TALEP_ADET', \n",
    "            'BILGI_TALEP_ADET', 'VADE_TUTAR_0', 'ODEME_TUTAR_0', 'VADE_TUTAR_1', 'ODEME_TUTAR_1', 'VADE_TUTAR_2', 'ODEME_TUTAR_2', \n",
    "            'VADE_TUTAR_3', 'ODEME_TUTAR_3', 'VADE_TUTAR_4', 'ODEME_TUTAR_4', 'VADE_TUTAR_5', 'ODEME_TUTAR_5', 'VADE_TUTAR_6', 'ODEME_TUTAR_6', \n",
    "            'VADE_TUTAR_7', 'ODEME_TUTAR_7', 'VADE_TUTAR_8', 'ODEME_TUTAR_8', 'VADE_TUTAR_9', 'ODEME_TUTAR_9', 'VADE_TUTAR_10','ODEME_TUTAR_10', \n",
    "            'VADE_TUTAR_11', 'ODEME_TUTAR_11', 'SON_AY_KATKI_MIKTARI', 'SON_AY_KATKI_ADET', 'SON_CEYREK_KATKI_MIKTARI','SON_CEYREK_KATKI_ADET', \n",
    "            'SON_SENE_KATKI_MIKTARI', 'SON_SENE_KATKI_ADET', 'ANAPARA', 'GETIRI', 'BU01', 'BU02', 'BU03', 'BU04', 'BU05', 'BU06', 'BU07', \n",
    "            'BU08', 'BU09', 'BU10', 'BU11', 'BU12', 'BU13', 'BU14', 'BU15', 'BU16', 'BU17', 'BU18', 'BU19', 'BU20', 'BU21','BU22', 'BU23', \n",
    "            'BU24', 'HU01', 'HU02', 'HU03', 'HU04', 'HU05', 'HU06', 'HU07', 'HU10', 'HU11', 'HU12', 'HU13', 'HU14', 'HU15', 'HU16', 'HU17',\n",
    "            'HU18', 'HU19', 'AKTIF_ILK_POLICE_RG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[cat_cols] = all_data[cat_cols].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "num_imputer = SimpleImputer(strategy='constant', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[num_cols] = num_imputer.fit_transform(all_data[num_cols])\n",
    "all_data[cat_cols] = cat_imputer.fit_transform(all_data[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rare_encoder(dataframe, cat_cols, rare_perc):\n",
    "    temp_df = dataframe.copy()\n",
    "\n",
    "    rare_columns = [col for col in cat_cols if (temp_df[col].value_counts() / len(temp_df) < rare_perc).any(axis=None)]\n",
    "\n",
    "    for var in rare_columns:\n",
    "        tmp = temp_df[var].value_counts() / len(temp_df)\n",
    "        rare_labels = tmp[tmp < rare_perc].index\n",
    "        temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n",
    "\n",
    "    return temp_df\n",
    "\n",
    "all_data = rare_encoder(all_data, cat_cols, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols.remove('PP_MESLEK')\n",
    "all_data_final = pd.concat([all_data.drop(columns=cat_cols), pd.get_dummies(all_data[cat_cols])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Meslek` özelliğine target encoding uyguladık."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Assuming 'all_data_final' is your DataFrame and it's already loaded\n",
    "y = all_data_final['LABEL']  # Target variable\n",
    "\n",
    "# Initialize the target encoder\n",
    "encoder = TargetEncoder()\n",
    "\n",
    "# Apply TargetEncoder to each column in cat_but_car list and replace the original column with encoded values\n",
    "all_data_final['PP_MESLEK'] = encoder.fit_transform(all_data_final['PP_MESLEK'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'all_data_final' is your DataFrame\n",
    "\n",
    "# Extracting HU1 to HU19 features\n",
    "hu_features = all_data_final.filter(regex='^HU')\n",
    "\n",
    "# Identifying BU features by filtering column names that start with 'BU'\n",
    "bu_features = all_data_final.filter(regex='^BU')\n",
    "\n",
    "# Combining HU and BU features\n",
    "combined_binary_features = pd.concat([hu_features, bu_features], axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "combined_binary_features_scaled = scaler.fit_transform(combined_binary_features)\n",
    "\n",
    "# Initialize PCA, specifying we want to keep enough components to explain 95% of the variance\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the combined binary features\n",
    "combined_binary_features_pca = pca.fit_transform(combined_binary_features_scaled)\n",
    "\n",
    "# Creating new feature names for the principal components\n",
    "new_feature_names = ['PC_1', 'PC_2']\n",
    "\n",
    "# Adding the principal components as new features to the original DataFrame\n",
    "all_data_final[new_feature_names] = combined_binary_features_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preped = all_data_final.iloc[:train.shape[0]]\n",
    "test_preped = all_data_final.iloc[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preped.to_csv('data/train_preped.csv', index=False)\n",
    "test_preped.to_csv('data/test_preped.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LABEL\n",
       "-1.0    0.976605\n",
       " 4.0    0.012611\n",
       " 1.0    0.004372\n",
       " 0.0    0.003727\n",
       " 6.0    0.000890\n",
       " 3.0    0.000793\n",
       " 2.0    0.000515\n",
       " 5.0    0.000487\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preped.LABEL.value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
